{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook scrapes text from Wiadomosci from February 2022 until December 2023. As all content up to 20th December 2023 had been deleted from the official website, the content of ‘Wiadomości’, TVP’s main news programme, is therefore scraped from a GitHub channel daily saving its transcript (codziennatranskrypcjatvpis, 2024). Data is then subsetted to only involve LGBTQ+-related coverages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kajaadamczyk/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "urls = [\n",
    "    \"https://raw.githubusercontent.com/codziennatranskrypcjatvpis/codziennatranskrypcjatvpis.github.io/main/transcriptions_txt/Wiadomosci_06.02.2022.txt\",\n",
    "    \"https://raw.githubusercontent.com/codziennatranskrypcjatvpis/codziennatranskrypcjatvpis.github.io/main/transcriptions_txt/Wiadomosci_07.02.2022.txt\"\n",
    "]\n",
    "\n",
    "def main():\n",
    "\n",
    "    all_texts = []\n",
    "\n",
    "    for url in urls:\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()  # This line checks for any HTTP errors\n",
    "            text = response.text.strip()  # No need for BeautifulSoup as it's a text file, not HTML\n",
    "            all_texts.append(text)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping URL: {url}. {e}\")\n",
    "\n",
    "    data = {\n",
    "        \"URLs\": urls,\n",
    "        \"Texts\": all_texts,\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame.from_dict(data)\n",
    "    df.to_csv(\"wiadomosci.csv\", encoding=\"utf-8\", header=True, index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URLs</th>\n",
       "      <th>Texts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://raw.githubusercontent.com/codziennatra...</td>\n",
       "      <td>Mamy pierwszy medal. Dawid Kubacki wywalczył b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://raw.githubusercontent.com/codziennatra...</td>\n",
       "      <td>W Brukseli prezydent Andrzej Duda wraz z szefa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                URLs  \\\n",
       "0  https://raw.githubusercontent.com/codziennatra...   \n",
       "1  https://raw.githubusercontent.com/codziennatra...   \n",
       "\n",
       "                                               Texts  \n",
       "0  Mamy pierwszy medal. Dawid Kubacki wywalczył b...  \n",
       "1  W Brukseli prezydent Andrzej Duda wraz z szefa...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiadomosci_df = pd.read_csv(\"wiadomosci.csv\")\n",
    "wiadomosci_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL not found: https://raw.githubusercontent.com/codziennatranskrypcjatvpis/codziennatranskrypcjatvpis.github.io/main/transcriptions_txt/Wiadomosci_15.11.2023.txt\n",
      "URL not found: https://raw.githubusercontent.com/codziennatranskrypcjatvpis/codziennatranskrypcjatvpis.github.io/main/transcriptions_txt/Wiadomosci_16.11.2023.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def generate_urls(start_date, end_date):\n",
    "    base_url = \"https://raw.githubusercontent.com/codziennatranskrypcjatvpis/codziennatranskrypcjatvpis.github.io/main/transcriptions_txt/Wiadomosci_{}.txt\"\n",
    "    urls = []\n",
    "    current_date = start_date\n",
    "    while current_date <= end_date:\n",
    "        urls.append(base_url.format(current_date.strftime(\"%d.%m.%Y\")))\n",
    "        current_date += timedelta(days=1)\n",
    "    return urls\n",
    "\n",
    "def scrape_text_from_url(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Check for HTTP errors\n",
    "        text = response.text.strip()\n",
    "        return text\n",
    "    except requests.HTTPError as e:\n",
    "        if response.status_code == 404:\n",
    "            print(f\"URL not found: {url}\")\n",
    "        else:\n",
    "            print(f\"HTTP error occurred while scraping URL: {url}. {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while scraping URL: {url}. {e}\")\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    start_date = datetime(2022, 2, 6)\n",
    "    end_date = datetime(2023, 12, 19)\n",
    "    urls = generate_urls(start_date, end_date)\n",
    "\n",
    "    all_texts = []\n",
    "    for url in urls:\n",
    "        text = scrape_text_from_url(url)\n",
    "        if text is not None:\n",
    "            all_texts.append(text)\n",
    "\n",
    "    if all_texts:  # Only create DataFrame if there's data\n",
    "        data = {\n",
    "            \"URLs\": urls[:len(all_texts)],  # Truncate URLs to match the length of all_texts\n",
    "            \"Texts\": all_texts,\n",
    "        }\n",
    "\n",
    "        df = pd.DataFrame.from_dict(data)\n",
    "        df.to_csv(\"wiadomosci.csv\", encoding=\"utf-8\", header=True, index=False)\n",
    "    else:\n",
    "        print(\"No data scraped.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL not found: https://raw.githubusercontent.com/codziennatranskrypcjatvpis/codziennatranskrypcjatvpis.github.io/main/transcriptions_txt/Wiadomosci_15.11.2023.txt\n",
      "URL not found: https://raw.githubusercontent.com/codziennatranskrypcjatvpis/codziennatranskrypcjatvpis.github.io/main/transcriptions_txt/Wiadomosci_16.11.2023.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def generate_urls(start_date, end_date):\n",
    "    base_url = \"https://raw.githubusercontent.com/codziennatranskrypcjatvpis/codziennatranskrypcjatvpis.github.io/main/transcriptions_txt/Wiadomosci_{}.txt\"\n",
    "    urls = []\n",
    "    dates = []\n",
    "    current_date = start_date\n",
    "    while current_date <= end_date:\n",
    "        url = base_url.format(current_date.strftime(\"%d.%m.%Y\"))\n",
    "        urls.append(url)\n",
    "        dates.append(current_date.strftime(\"%Y-%m-%d\"))\n",
    "        current_date += timedelta(days=1)\n",
    "    return urls, dates\n",
    "\n",
    "def scrape_text_from_url(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Check for HTTP errors\n",
    "        text = response.text.strip()\n",
    "        return text\n",
    "    except requests.HTTPError as e:\n",
    "        if response.status_code == 404:\n",
    "            print(f\"URL not found: {url}\")\n",
    "        else:\n",
    "            print(f\"HTTP error occurred while scraping URL: {url}. {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while scraping URL: {url}. {e}\")\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    start_date = datetime(2022, 2, 6)\n",
    "    end_date = datetime(2023, 12, 19)\n",
    "    urls, dates = generate_urls(start_date, end_date)\n",
    "\n",
    "    all_texts = []\n",
    "    for url in urls:\n",
    "        text = scrape_text_from_url(url)\n",
    "        if text is not None:\n",
    "            all_texts.append(text)\n",
    "\n",
    "    if all_texts:  # Only create DataFrame if there's data\n",
    "        data = {\n",
    "            \"Date\": dates[:len(all_texts)],  # Truncate dates to match the length of all_texts\n",
    "            \"URLs\": urls[:len(all_texts)],  # Truncate URLs to match the length of all_texts\n",
    "            \"Texts\": all_texts,\n",
    "        }\n",
    "\n",
    "        df = pd.DataFrame.from_dict(data)\n",
    "        df.to_csv(\"wiadomosci.csv\", encoding=\"utf-8\", header=True, index=False)\n",
    "    else:\n",
    "        print(\"No data scraped.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>URLs</th>\n",
       "      <th>Texts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-02-06</td>\n",
       "      <td>https://raw.githubusercontent.com/codziennatra...</td>\n",
       "      <td>Mamy pierwszy medal. Dawid Kubacki wywalczył b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-02-07</td>\n",
       "      <td>https://raw.githubusercontent.com/codziennatra...</td>\n",
       "      <td>W Brukseli prezydent Andrzej Duda wraz z szefa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-02-08</td>\n",
       "      <td>https://raw.githubusercontent.com/codziennatra...</td>\n",
       "      <td>Rada Polityki Pieniężnej reaguje na inflację. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-02-09</td>\n",
       "      <td>https://raw.githubusercontent.com/codziennatra...</td>\n",
       "      <td>Apogeum pi@tej fali koronawirusa ju¼ za nami. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-02-10</td>\n",
       "      <td>https://raw.githubusercontent.com/codziennatra...</td>\n",
       "      <td>Ważne słowa i deklaracje brytyjskiego premiera...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date                                               URLs  \\\n",
       "0  2022-02-06  https://raw.githubusercontent.com/codziennatra...   \n",
       "1  2022-02-07  https://raw.githubusercontent.com/codziennatra...   \n",
       "2  2022-02-08  https://raw.githubusercontent.com/codziennatra...   \n",
       "3  2022-02-09  https://raw.githubusercontent.com/codziennatra...   \n",
       "4  2022-02-10  https://raw.githubusercontent.com/codziennatra...   \n",
       "\n",
       "                                               Texts  \n",
       "0  Mamy pierwszy medal. Dawid Kubacki wywalczył b...  \n",
       "1  W Brukseli prezydent Andrzej Duda wraz z szefa...  \n",
       "2  Rada Polityki Pieniężnej reaguje na inflację. ...  \n",
       "3  Apogeum pi@tej fali koronawirusa ju¼ za nami. ...  \n",
       "4  Ważne słowa i deklaracje brytyjskiego premiera...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiadomosci_df = pd.read_csv(\"wiadomosci.csv\")\n",
    "wiadomosci_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
